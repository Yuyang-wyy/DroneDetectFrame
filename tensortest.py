import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np
np.bool = np.bool_
import os
from PIL import Image
import torch
from torchvision import transforms
from scipy.special import softmax
import matplotlib.pyplot as plt

class TensorRTInference:
    def __init__(self, engine_path, verbose=False):
        """åˆå§‹åŒ–TensorRTæ¨ç†å™¨"""
        print(f"ğŸ”§ åˆå§‹åŒ–TensorRTæ¨ç†å™¨...")
        print(f"å¼•æ“è·¯å¾„: {engine_path}")
        
        self.verbose = verbose
        self.logger = trt.Logger(trt.Logger.VERBOSE if verbose else trt.Logger.WARNING)
        
        # åŠ è½½å¼•æ“
        self.engine = self.load_engine(engine_path)
        self.context = self.engine.create_execution_context()
        
        # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
        self.get_engine_info()
        
        # åˆ†é…GPUå†…å­˜
        self.allocate_buffers()
        
        # æ•°æ®é¢„å¤„ç† - ä¸PyTorchä¸€è‡´
        self.image_transform = transforms.Compose([
            transforms.Resize((256, 320)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        print("âœ… TensorRTæ¨ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_engine(self, engine_path):
        """åŠ è½½TensorRTå¼•æ“"""
        if not os.path.exists(engine_path):
            raise FileNotFoundError(f"å¼•æ“æ–‡ä»¶ä¸å­˜åœ¨: {engine_path}")
        
        print("ğŸ“‚ åŠ è½½TensorRTå¼•æ“...")
        runtime = trt.Runtime(self.logger)
        with open(engine_path, 'rb') as f:
            engine = runtime.deserialize_cuda_engine(f.read())
        
        if engine is None:
            raise RuntimeError("å¼•æ“åŠ è½½å¤±è´¥")
        
        print("âœ… TensorRTå¼•æ“åŠ è½½æˆåŠŸ")
        return engine
    
    def get_engine_info(self):
        """è·å–å¼•æ“è¾“å…¥è¾“å‡ºä¿¡æ¯"""
        self.input_name = None
        self.output_name = None
        self.input_dtype = None
        self.output_dtype = None
        
        for i in range(self.engine.num_io_tensors):
            name = self.engine.get_tensor_name(i)
            dtype = self.engine.get_tensor_dtype(name)
            mode = self.engine.get_tensor_mode(name)
            
            if mode == trt.TensorIOMode.INPUT:
                self.input_name = name
                self.input_dtype = trt.nptype(dtype)
            else:
                self.output_name = name
                self.output_dtype = trt.nptype(dtype)
    
    def allocate_buffers(self, max_batch_size=1):
        """åˆ†é…GPUå†…å­˜ç¼“å†²åŒº"""
        self.inputs = []
        self.outputs = []
        self.bindings = []
        
        for i in range(self.engine.num_io_tensors):
            name = self.engine.get_tensor_name(i)
            dtype = trt.nptype(self.engine.get_tensor_dtype(name))
            
            if self.engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:
                size = max_batch_size * 3 * 256 * 320
                device_mem = cuda.mem_alloc(size * np.dtype(dtype).itemsize)
                self.inputs.append({'name': name, 'device': device_mem, 'dtype': dtype})
            else:
                size = max_batch_size * 2 * 512 * 640  # 2 classes, output resolution
                device_mem = cuda.mem_alloc(size * np.dtype(dtype).itemsize)
                host_mem = cuda.pagelocked_empty(size, dtype)
                self.outputs.append({'name': name, 'host': host_mem, 'device': device_mem, 'dtype': dtype})
            
            self.bindings.append(int(device_mem))
    
    def preprocess_image(self, image_path):
        """é¢„å¤„ç†å›¾åƒ"""
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.image_transform(image)
        return image_tensor.unsqueeze(0).numpy()
    
    def generate_heatmap(self, input_dir, output_dir):
        """ç”Ÿæˆçƒ­åŠ›å›¾"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # è·å–è¾“å…¥å›¾åƒ
        image_files = sorted([f for f in os.listdir(input_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
        print(f"Found {len(image_files)} images in {input_dir}")
        
        # é¢„çƒ­å¼•æ“
        print("ğŸ”¥ é¢„çƒ­TensorRTå¼•æ“...")
        dummy_input = np.random.randn(1, 3, 256, 320).astype(self.input_dtype)
        self.context.set_input_shape(self.input_name, (1, 3, 256, 320))
        for _ in range(10):
            cuda.memcpy_htod(self.inputs[0]['device'], dummy_input)
            self.context.execute_v2(self.bindings)
            cuda.memcpy_dtoh(self.outputs[0]['host'], self.outputs[0]['device'])
        print("âœ… é¢„çƒ­å®Œæˆ")
        
        # é€å¼ å›¾åƒæ¨ç†
        for image_file in image_files:
            # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
            img_path = os.path.join(input_dir, image_file)
            try:
                image_tensor = self.preprocess_image(img_path)
            except Exception as e:
                print(f"Error loading {img_path}: {e}")
                continue
            
            # è®¾ç½®è¾“å…¥å½¢çŠ¶
            self.context.set_input_shape(self.input_name, (1, 3, 256, 320))
            
            # å¤åˆ¶è¾“å…¥æ•°æ®åˆ°GPU
            cuda.memcpy_htod(self.inputs[0]['device'], image_tensor)
            
            # æ¨ç†
            success = self.context.execute_v2(self.bindings)
            if not success:
                print(f"âŒ TensorRTæ¨ç†å¤±è´¥ for {image_file}")
                continue
            
            # å¤åˆ¶è¾“å‡ºæ•°æ®åˆ°CPU
            cuda.memcpy_dtoh(self.outputs[0]['host'], self.outputs[0]['device'])
            
            # åå¤„ç†
            output_shape = self.context.get_tensor_shape(self.output_name)
            batch_output = self.outputs[0]['host'][:np.prod(output_shape)].reshape(output_shape)
            batch_probs = softmax(batch_output, axis=1)  # å½¢çŠ¶ä¸º (1, num_classes, height, width)
            foreground_probs = batch_probs[0, 1, :, :]  # å‰æ™¯ç±»æ¦‚ç‡ï¼Œå½¢çŠ¶ (height, width)
            
            # ç”Ÿæˆå¹¶ä¿å­˜çƒ­åŠ›å›¾
            plt.figure(figsize=(8, 6))
            plt.imshow(foreground_probs, cmap='hot', vmin=0, vmax=1)
            plt.colorbar(label='Foreground Probability')
            plt.title(f'Heatmap: {image_file}')
            plt.axis('off')
            
            # ä¿å­˜çƒ­åŠ›å›¾
            heatmap_path = os.path.join(output_dir, f"heatmap_{image_file}")
            plt.savefig(heatmap_path, bbox_inches='tight', pad_inches=0.1)
            plt.close()
            print(f"Saved heatmap for {image_file} at {heatmap_path}")
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        try:
            for inp in self.inputs:
                if hasattr(inp['device'], 'free'):
                    inp['device'].free()
            for out in self.outputs:
                if hasattr(out['device'], 'free'):
                    out['device'].free()
        except:
            pass

# ä¸»ç¨‹åº
if __name__ == '__main__':
    # é…ç½®å‚æ•°
    config = {
        'engine_path': './ultralight_segmentation_256x320_fp16.trt',
        'input_dir': 'E:/Robotics/Work/cv/newpics',
        'output_dir': 'E:/Robotics/Work/cv/deep_final_results/tensorheatmap',
    }
    
    print("=" * 60)
    print("ğŸš€ TensorRT DeepLabv3+ çƒ­åŠ›å›¾ç”Ÿæˆ")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ¨ç†å™¨
    try:
        inferencer = TensorRTInference(config['engine_path'], verbose=True)
    except Exception as e:
        print(f"âŒ TensorRTæ¨ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        exit()
    
    # ç”Ÿæˆçƒ­åŠ›å›¾
    inferencer.generate_heatmap(config['input_dir'], config['output_dir'])
    print(f"Heatmaps saved in {config['output_dir']}")